{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":309628,"datasetId":107582,"databundleVersionId":322619,"isSourceIdPinned":false}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Week 14: CNN Lab - Rock, Paper, Scissors\n\n**Objective:** Build, train, and test a Convolutional Neural Network (CNN) to classify images of hands playing Rock, Paper, or Scissors.","metadata":{"id":"intro_cell"}},{"cell_type":"markdown","source":"### Step 1: Setup and Data Download\n\nThis first cell downloads the dataset from Kaggle.","metadata":{"id":"step_1_md"}},{"cell_type":"code","source":"import kagglehub\n\npath = kagglehub.dataset_download(\"drgfreeman/rockpaperscissors\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"id":"DHpeGiatx8yO","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T16:00:39.227953Z","iopub.execute_input":"2025-11-20T16:00:39.228279Z","iopub.status.idle":"2025-11-20T16:01:05.206330Z","shell.execute_reply.started":"2025-11-20T16:00:39.228255Z","shell.execute_reply":"2025-11-20T16:01:05.205225Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/rockpaperscissors\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import shutil\nimport os\n\nsrc_root = \"/kaggle/input/rockpaperscissors\"\ndst_root = \"/content/dataset\"\n\nos.makedirs(dst_root, exist_ok=True)\n\nfolders_to_copy = [\"rock\", \"paper\", \"scissors\"]\n\nfor folder in folders_to_copy:\n    src_path = os.path.join(src_root, folder)\n    dst_path = os.path.join(dst_root, folder)\n\n    if os.path.exists(src_path):\n        shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n        print(\"Copied:\", folder)\n    else:\n        print(\"Folder not found:\", folder)\n\n","metadata":{"id":"3SVJhchl2XCb","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T16:01:32.804444Z","iopub.execute_input":"2025-11-20T16:01:32.804745Z","iopub.status.idle":"2025-11-20T16:01:54.182935Z","shell.execute_reply.started":"2025-11-20T16:01:32.804722Z","shell.execute_reply":"2025-11-20T16:01:54.182168Z"}},"outputs":[{"name":"stdout","text":"Copied: rock\nCopied: paper\nCopied: scissors\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Step 2: Imports and Device Setup\n\nImport the necessary libraries and check if a GPU is available.","metadata":{"id":"step_2_md"}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom PIL import Image\nimport numpy as np\n\n# TODO: Set the 'device' variable\n# Check if CUDA (GPU) is available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Using device:\", device)","metadata":{"id":"1n2gYN8TyydM","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T16:02:35.317919Z","iopub.execute_input":"2025-11-20T16:02:35.318625Z","iopub.status.idle":"2025-11-20T16:02:44.068201Z","shell.execute_reply.started":"2025-11-20T16:02:35.318597Z","shell.execute_reply":"2025-11-20T16:02:44.067325Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Step 3: Data Loading and Preprocessing\n\nHere we will define our image transformations, load the dataset, split it, and create DataLoaders.","metadata":{"id":"step_3_md"}},{"cell_type":"code","source":"DATA_DIR = \"/content/dataset\"\n\n# TODO: Define the image transforms\n# We need to:\n# 1. Resize all images to 128x128\n# 2. Convert them to Tensors\n# 3. Normalize them (mean=0.5, std=0.5)\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\n# Load dataset using ImageFolder\nfull_dataset = datasets.ImageFolder(DATA_DIR, transform=transform)\n\nclass_names = full_dataset.classes\nprint(\"Classes:\", class_names)\n\n# TODO: Split the dataset\n# We want 80% for training and 20% for testing\ntrain_size = int(0.8 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\n\n# TODO: Use random_split to create train_dataset and test_dataset\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# TODO: Create the DataLoaders\n# Use a batch_size of 32\n# Shuffle the training loader, but not the test loader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nprint(f\"Total images: {len(full_dataset)}\")\nprint(f\"Training images: {len(train_dataset)}\")\nprint(f\"Test images: {len(test_dataset)}\")","metadata":{"id":"SkJ5XlSDy0HF","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T16:03:21.436325Z","iopub.execute_input":"2025-11-20T16:03:21.436731Z","iopub.status.idle":"2025-11-20T16:03:21.455420Z","shell.execute_reply.started":"2025-11-20T16:03:21.436709Z","shell.execute_reply":"2025-11-20T16:03:21.454415Z"}},"outputs":[{"name":"stdout","text":"Classes: ['paper', 'rock', 'scissors']\nTotal images: 2188\nTraining images: 1750\nTest images: 438\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Step 4: Define the CNN Model\n\nFill in the `conv_block` and `fc_block` with the correct layers.","metadata":{"id":"step_4_md"}},{"cell_type":"code","source":"class RPS_CNN(nn.Module):\n    def __init__(self):\n        super(RPS_CNN, self).__init__()\n\n        # TODO: Define the convolutional block\n        # We want 3 blocks:\n        # 1. Conv2d(3 -> 16 channels, kernel=3, padding=1), ReLU, MaxPool2d(2)\n        # 2. Conv2d(16 -> 32 channels, kernel=3, padding=1), ReLU, MaxPool2d(2)\n        # 3. Conv2d(32 -> 64 channels, kernel=3, padding=1), ReLU, MaxPool2d(2)\n        self.conv_block = nn.Sequential(\n            # Block 1\n            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            # Block 2\n            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            # Block 3\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n\n        # After 3 MaxPool(2) layers, our 128x128 image becomes:\n        # 128 -> 64 -> 32 -> 16\n        # So the flattened size is 64 * 16 * 16\n\n        # TODO: Define the fully-connected (classifier) block\n        # We want:\n        # 1. Flatten the input\n        # 2. Linear layer (64 * 16 * 16 -> 256)\n        # 3. ReLU\n        # 4. Dropout (p=0.3)\n        # 5. Linear layer (256 -> 3) (3 classes: rock, paper, scissors)\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 16 * 16, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 3)\n        )\n\n    def forward(self, x):\n        x = self.conv_block(x)\n        x = self.fc(x)\n        return x\n\n# TODO: Initialize the model, criterion, and optimizer\n# 1. Create an instance of RPS_CNN and move it to the 'device'\nmodel = RPS_CNN().to(device)\n\n# 2. Define the loss function (Criterion). Use CrossEntropyLoss for classification.\ncriterion = nn.CrossEntropyLoss()\n\n# 3. Define the optimizer. Use Adam with a learning rate of 0.001\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nprint(model)","metadata":{"id":"wexPK8V3y3Fx","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T16:04:04.334997Z","iopub.execute_input":"2025-11-20T16:04:04.335415Z","iopub.status.idle":"2025-11-20T16:04:04.397765Z","shell.execute_reply.started":"2025-11-20T16:04:04.335387Z","shell.execute_reply":"2025-11-20T16:04:04.396738Z"}},"outputs":[{"name":"stdout","text":"RPS_CNN(\n  (conv_block): Sequential(\n    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU()\n    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=16384, out_features=256, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.3, inplace=False)\n    (4): Linear(in_features=256, out_features=3, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Step 5: Train the Model\n\nFill in the core training steps inside the loop.","metadata":{"id":"step_5_md"}},{"cell_type":"code","source":"EPOCHS = 10\n\nfor epoch in range(EPOCHS):\n    model.train() # Set the model to training mode\n    total_loss = 0\n\n    for images, labels in train_loader:\n        # Move data to the correct device\n        images, labels = images.to(device), labels.to(device)\n\n        # TODO: Implement the training steps\n        # 1. Clear the gradients (optimizer.zero_grad())\n        optimizer.zero_grad()\n\n        # 2. Perform a forward pass (get model outputs)\n        outputs = model(images)\n\n        # 3. Calculate the loss (using criterion)\n        loss = criterion(outputs, labels)\n\n        # 4. Perform a backward pass (loss.backward())\n        loss.backward()\n\n        # 5. Update the weights (optimizer.step())\n        optimizer.step()\n\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss = {total_loss/len(train_loader):.4f}\")\n\nprint(\"Training complete!\")","metadata":{"id":"L_GqO57IzQLs","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T16:06:11.679209Z","iopub.execute_input":"2025-11-20T16:06:11.680129Z","iopub.status.idle":"2025-11-20T16:09:50.300860Z","shell.execute_reply.started":"2025-11-20T16:06:11.680054Z","shell.execute_reply":"2025-11-20T16:09:50.299867Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss = 0.6905\nEpoch 2/10, Loss = 0.2427\nEpoch 3/10, Loss = 0.1077\nEpoch 4/10, Loss = 0.0426\nEpoch 5/10, Loss = 0.0349\nEpoch 6/10, Loss = 0.0156\nEpoch 7/10, Loss = 0.0130\nEpoch 8/10, Loss = 0.0111\nEpoch 9/10, Loss = 0.0074\nEpoch 10/10, Loss = 0.0070\nTraining complete!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Step 6: Evaluate the Model\n\nTest the model's accuracy on the unseen test set.","metadata":{"id":"step_6_md"}},{"cell_type":"code","source":"model.eval() # Set the model to evaluation mode\ncorrect = 0\ntotal = 0\n\n# TODO: Use torch.no_grad()\n# We don't need to calculate gradients during evaluation\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        # TODO: Get model predictions\n        # 1. Get the raw model outputs (logits)\n        outputs = model(images)\n\n        # 2. Get the predicted class (the one with the highest score)\n        #    Hint: use torch.max(outputs, 1)\n        _, predicted = torch.max(outputs, 1)\n\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")","metadata":{"id":"iYCNxBrjzU1t","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T16:10:29.033541Z","iopub.execute_input":"2025-11-20T16:10:29.033889Z","iopub.status.idle":"2025-11-20T16:10:32.287424Z","shell.execute_reply.started":"2025-11-20T16:10:29.033864Z","shell.execute_reply":"2025-11-20T16:10:32.286408Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 97.95%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Step 7: Test on a Single Image\n\nLet's see how the model performs on one image.","metadata":{"id":"step_7_md"}},{"cell_type":"code","source":"def predict_image(model, img_path):\n    model.eval()\n\n    img = Image.open(img_path).convert(\"RGB\")\n    # Apply the same transforms as training, and add a batch dimension (unsqueeze)\n    img = transform(img).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        # TODO: Get the model prediction\n        # 1. Get the raw model outputs (logits)\n        output = model(img)\n\n        # 2. Get the predicted class index\n        _, pred = torch.max(output, 1)\n\n    return class_names[pred.item()]\n\n# Test the function (this path should exist)\ntest_img_path = \"/content/dataset/paper/0Uomd0HvOB33m47I.png\"\ntry:\n    prediction = predict_image(model, test_img_path)\n    print(f\"Model prediction for {test_img_path}: {prediction}\")\nexcept FileNotFoundError:\n    print(f\"Could not find image at {test_img_path}. Please check the path.\")","metadata":{"id":"RN00Dkw9zceh","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T16:10:36.372083Z","iopub.execute_input":"2025-11-20T16:10:36.372474Z","iopub.status.idle":"2025-11-20T16:10:36.391907Z","shell.execute_reply.started":"2025-11-20T16:10:36.372447Z","shell.execute_reply":"2025-11-20T16:10:36.390830Z"}},"outputs":[{"name":"stdout","text":"Model prediction for /content/dataset/paper/0Uomd0HvOB33m47I.png: paper\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Step 8: Play the Game!\n\nThis code is complete. If your model is trained, you can run this cell to have the model play against itself.","metadata":{"id":"step_8_md"}},{"cell_type":"code","source":"import random\nimport os\n\ndef pick_random_image(class_name):\n    folder = f\"/content/dataset/{class_name}\"\n    files = os.listdir(folder)\n    img = random.choice(files)\n    return os.path.join(folder, img)\n\ndef rps_winner(move1, move2):\n    if move1 == move2:\n        return \"Draw\"\n\n    rules = {\n        \"rock\": \"scissors\",\n        \"paper\": \"rock\",\n        \"scissors\": \"paper\"\n    }\n\n    if rules[move1] == move2:\n        return f\"Player 1 wins! {move1} beats {move2}\"\n    else:\n        return f\"Player 2 wins! {move2} beats {move1}\"\n\n\n# -----------------------------------------------------------\n# 1. Choose any two random classes\n# -----------------------------------------------------------\n\nchoices = [\"rock\", \"paper\", \"scissors\"]\nc1 = random.choice(choices)\nc2 = random.choice(choices)\n\nimg1_path = pick_random_image(c1)\nimg2_path = pick_random_image(c2)\n\nprint(\"Randomly selected images:\")\nprint(\"Image 1:\", img1_path)\nprint(\"Image 2:\", img2_path)\n\n\n# -----------------------------------------------------------\n# 2. Predict their labels using the model\n# -----------------------------------------------------------\n\np1 = predict_image(model, img1_path)\np2 = predict_image(model, img2_path)\n\nprint(\"\\nPlayer 1 shows:\", p1)\nprint(\"Player 2 shows:\", p2)\n\n# -----------------------------------------------------------\n# 3. Decide the winner\n# -----------------------------------------------------------\n\nprint(\"\\nRESULT:\", rps_winner(p1, p2))","metadata":{"id":"13-RXEbuzuxu","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T16:10:40.911450Z","iopub.execute_input":"2025-11-20T16:10:40.911766Z","iopub.status.idle":"2025-11-20T16:10:40.939562Z","shell.execute_reply.started":"2025-11-20T16:10:40.911741Z","shell.execute_reply":"2025-11-20T16:10:40.938709Z"}},"outputs":[{"name":"stdout","text":"Randomly selected images:\nImage 1: /content/dataset/scissors/JGKvhDVwPPH5n7Kp.png\nImage 2: /content/dataset/rock/k6r5z3dMsqo2H6hk.png\n\nPlayer 1 shows: scissors\nPlayer 2 shows: rock\n\nRESULT: Player 2 wins! rock beats scissors\n","output_type":"stream"}],"execution_count":9}]}